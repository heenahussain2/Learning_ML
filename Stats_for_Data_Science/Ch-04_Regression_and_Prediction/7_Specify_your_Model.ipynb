{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify Your Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "- Model specification is the process of determining which independent variables belong in the model and whether modeling curvature and interaction effects are appropriate.\n",
    "- This chapter covers statistical methods, difficulties that can arise and practical suggestions for selecting the model.\n",
    "- During the specification process you have to try different combinations of variables and various forms of the model. For example you can try different terms that explain interactions between variables and curvature in the data.\n",
    "- We need to reach a Goldilocks balance by including the correct number of independent variables in the regression equation.\n",
    "    - **Too few** : Underspecified models tend to be biased.\n",
    "    - **Too many**: Overspecified models tend to be less precise.\n",
    "    - **Just Right**: Models with the correct terms are not biased and are the most precise.\n",
    "- If a study wants to test a particular relationship, then the regression equation should contain the independent variables that we are explicitly testing, along with other variables that affect the dependent variable..\n",
    "- This process allows the regression model to assess the study's main questions while controlling for other variables that can influence the dependent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Importance of Graphing Your Data\n",
    "- When you start working with a dataset to build a regression model, the first thing you should do is graph your data.\n",
    "- This will help you learn a lot about your data and the relationship between variables.\n",
    "- For regression analysis, where most data are continuous, scatterplots are crucial. Scatterplots will show you whether there are positive or negative relationships and if they are linear or curvilinear.\n",
    "- When some relationships are curved, the shape of the curve in the scatterplot might provide ideas about how to model it.\n",
    "- We can also calculate the correlations between the candidate independent variables and the dependent variable. Significant correlations suggest that we should consider including those variables in the model.\n",
    "- We can include categorical variables in scatterplots to determine whether those variables play a role. Alternatively, we can graph the dependent variable by groups using boxplots or individual value plots.\n",
    "- If working on multiple regression model, we can use scatterplot matrix to display numerous relationships at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistical Methods for Model Specification\n",
    "- We can use statistical assessments during the model specification process.\n",
    "- Various metrics and algorithms can help you determine which independent variables to include in the regression equation.\n",
    "\n",
    "#### Adjusted R-squared and Predicted R-squared\n",
    "- Typically you want to select models that have larger adjusted and predicted R-squared values. These statistics help avoid the fundamental problems with R-squared - it always increases when you add an independent variable.\n",
    "- This property tempts us into specifying a model that is too complex, and which leads to overfitting, producing misleading results.\n",
    "- **Adjusted R-squared** increases only when a new variable improves the model by more than chance. Low-quality variables can cause it to decrease.\n",
    "- **Predicted R-squared** is a cross-validation method that can also decrease. Cross-validation partitions your data to determine whether the model is generalizable outside of your dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mallows' Cp\n",
    "- Mallows' Cp helps you choose between multiple regression models by striking a balance between precision and bias.\n",
    "- We want to include a sufficient number of independent variables to eliminate bias but not too many to reduce precision. This balance changes with the number of independent variables in the model.\n",
    "- Mallows' Cp compares the precision and bias of the full model to models with a subset of predictors.\n",
    "- Typically, you want Mallows' Cp to be small and close to the number of variables in the models + constant.\n",
    "- A Mallows' Cp value that meets these criteria suggests that the coefficient estimates are both relatively precise (small variance) and unbiased.\n",
    "- Biased models have larger values of Mallows' Cp."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### P-values for the independent variables\n",
    "- In regression, p-values less than the significance level indicate that the term is statistically significant. When a variable is not significant, consider removing it from the model.\n",
    "- **\"Reducing the model\"** is the process of including all candidate variables in the model, and then repeatedly removing the single term with the highest non-significant p-value until your model contains only significant terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stepwise regression and Best subsets regression\n",
    "- These two automated model selection procedures are algorithms that pick the variables to include in your regression equation.\n",
    "- These automated methods are helpful when you have too many independent variables, and you need some help in the investigative stages of the variable selection process.\n",
    "- These procedures can provide the Mallows' Cp statistic, which helps you balance the tradeoff between precision and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Complications\n",
    "- There are a few complications that can arise from using statistical methods for model selection. They are as follows.\n",
    "- Your best model is only as good as the data you collect.\n",
    "    - Specification of the correct model depends on measuring the proper variables.\n",
    "    - When you omit important variables from the model, the estimates for the variables that you include can be biased.\n",
    "    - This condition is known as **omitted variable bias**.\n",
    "- The sample you collect can be unusual, either by luck or methodology.\n",
    "    - False discoveries and false negatives are inevitable when you work with samples.\n",
    "- Multicolinearity occurs when independent variables in a regression equation are correlated.\n",
    "    - When multicolinearity is present, small changes in the equation can produce dramatic changes in coefficients and p-values.\n",
    "    - It can also reduce statistical significance in variables that are relevant.\n",
    "    - Due to these reasons, multicolinearity makes model selection challenging.\n",
    "- If you fit many models during the selection process.\n",
    "    - You will find variables appear to be statistically significant, but they are correlated only by chance.\n",
    "    - This problem occurs because all hypothesis tests have a false discovery rate.\n",
    "    - This type of data mining can make even random data appear to have significant relationships.\n",
    "- P-values, adjusted R-squared, predicted R-squared, and Mallows' Cp can point to different regression equations and there is no clear answer.\n",
    "- Stepwise regression and best subsets regression.\n",
    "    - These are automated model selection procedures that can help you in the early stages of model specification.\n",
    "    - These tools can get close to the right answer but they usually don't specify the correct model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Recommendations (for above problems)\n",
    "- Model Specification is as much a science as it is an art.\n",
    "- Statistical methods can help, but ultimately you'll need to place a high weight on theory and other considerations.\n",
    "\n",
    "#### Theory\n",
    "- The best practice is to review the literature to develop a theoretical understanding of the relevant independent variables, their relationships with the dependent variable, and the expected coefficient signs and effect magnitudes before you begin collecting data.\n",
    "- Building your knowledge helps you collect the correct data in the first place and it helps you specify the best regression equation without resorting to data mining.\n",
    "- Specification should not be based only on statistical measures. In fact, the foundation of your model selection process should depend largely on theoretical concerns.\n",
    "- Be sure to determine if the statistical results match theory and, if necessary make adjustments.\n",
    "- For ex - if theory suggests that an independent variable is important, you might include it in the regression equation even when its p-value is not significant.\n",
    "- If the coefficient sign is opposite of the theory, investigate and either modify the model or explain the inconsistency.\n",
    "\n",
    "#### Simplicity\n",
    "- People often think that complex problems require complicated regression equations. However simplification usually produces more precise models.\n",
    "- When you have several models with similar predictive power, choose the simplest as it is likely to be the best model.\n",
    "- Start simple and then add complexity only when it is actually needed.\n",
    "- As you make a model more complex, it becomes more likely that you are tailoring it to fit the quirks in your particular dataset rather than actual relationships in the population. \n",
    "- This causes overfitting which reduces generalizability and can produce results you can't trust.\n",
    "\n",
    "#### Residual Plots\n",
    "- During the specification process, check the residual plots. Residual plots are an easy way to avoid biased models and can help you make adjustments.\n",
    "- For instance, residual plots display patterns when an underspecified regression equation is biased, which can indicate the need to model curvature.\n",
    "- The simplest model that creates random residuals is a great contender for being reasonably precise and unbiased.\n",
    "\n",
    "#### In Short\n",
    "- Ultimately, statistical measures can't tell you which regression equation is best. They don't understand the fundamentals of the subject area.\n",
    "- Subject area expertise is always a vital part of the model specification process.\n",
    "- We want simple models that you choose based on theory.\n",
    "- Its tempting to try many combinations of variables to find the best model but thats not the best approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Omitted Variable Bias\n",
    "- Omitted variable bias occurs when a regression model leaves out relevant independent variables, which are known as confounding variables.\n",
    "- This condition forces the model to attribute the effects of omitted variables to the variables that are in the model, which biases the coefficient estimates.\n",
    "- This problem occurs because your linear regression model is specified incorrectly. This is because either the confounding variables are unknown or because the data does not exist.\n",
    "- If this bias affects your model, it is a severe condition because you can't trust your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Effects of Omitted Variable Bias\n",
    "- Omitting confounding variables from your regression model can bias coefficient estimates.\n",
    "- When you're assessing the regression coefficients in the statistical output, this bias can produce the following problems:\n",
    "    - Overestimate the strength of an effect.\n",
    "    - Underestimate the strength of an effect.\n",
    "    - Change the sign of an effect.\n",
    "    - Mask an effect that actually exists.\n",
    "\n",
    "#### Synonyms for Confounding Variables and Omitted Variable Bias.\n",
    "- **Omitted variables** that cause bias are also referred to as confounding variables, confounders and lurking variables.\n",
    "- These are important variables that the statistical model does not include and, therefore cannot control.\n",
    "- The **omitted variable bias** is referred to as spurious effects and spurious relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditions that cause Omitted Variable Bias\n",
    "- For omitted variable bias to occur, the following two conditions must exist:\n",
    "    1. The omitted variable must correlate with the dependent variable.\n",
    "    2. The omitted variable must correlate with at least one independent variable that is in the regression model.\n",
    "- This correlation structure causes confounding variables that are not in the model to bias the estimates that appear in your regression results.\n",
    "- The amount of bias depends on the strength of these correlations.\n",
    "- Strong correlations produce greater bias. If the relationships are weak, then the bias might not be severe.\n",
    "- If the omitted variable is not correlated with another independent variable at all, excluding it does not produce bias.\n",
    "- Omitted variable bias tends to occur in observational studies. Randomized studies minimize the effects of confounding variables by equally distributing them across the treatment groups, so the bias is less likely to be a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations, Residuals and OLS Assumptions\n",
    "- When you satisfy the ordinary least squares (OLS) assumptions, the Gauss-Markov theorem states that your estimates will be unbiased and have minimum variance.\n",
    "- Omitted variable bias occurs because the residuals violate one of the assumptions.\n",
    "- To see how this works we need to follow a chain of events.\n",
    "- Suppose you have a regression model with two significant independent variables, X1 and X2. These independent variables correlate with each other and the dependent variable - which are the requirements for omitted variable bias.\n",
    "- Now if we take X2 out of the model, here's what happens:\n",
    "    1. The model fits the data less well because we have removed a significant explanatory variable. Consequently the gap between the observed values and the fitted values increases. These gaps are the residuals.\n",
    "    2. The degree to which each residual increases depends on the relationship between X2 and the dependent variable. Consequently, the residuals correlate with X2.\n",
    "    3. X1 correlates with X2, and X2 correlates with the residuals. Ergo, variable X1 correlates with the residuals.\n",
    "    4. Hence, this condition violates the ordinary least squares assumption that independent variables in the model do not correlate with the residuals. Violations of this assumption produce biased estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detect Omitted Variable Bias and Identify Confounding Variables\n",
    "- If you include different combinations of independent variables in the model, and you see the coefficients changing, then it is omitted variable bias in action.\n",
    "- We know that for omitted variable bias to exist, an independent variable must correlate with the residuals.\n",
    "- Consequently, we can plot the residuals by the variables in our model. If we see a relationship in the plot, rather than random scatter, it both tells us that there is a problem and also points towards its solution.\n",
    "- We know which independent variable correlates to the confounding variable (the residuals will have a pattern for that variable). This can help you track the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obstacles to Correcting Omitted Variable Bias\n",
    "- The best correction for omitted variable bias is including the variable in the model.\n",
    "- This allows the regression model to control for the missing variable and prevent the spurious effects that the omitted variable might have caused otherwise.\n",
    "- Theoretically, you should include all independent variables that have a relationship with the dependent variable, but this approach produces real-world problems.\n",
    "    1. You might need to collect data on many more characteristics than is feasible. Additionally, some of these characteristics might be very difficult or even impossible to measure (for ex- some theoretical concept like ability).\n",
    "    2. As you include more variables in the model, the number of observations must increase to avoid overfitting the model, which can also produce unreliable results. Measuring  more characteristics and gathering a larger sample size can be very expensive.\n",
    "    3. Since the bias occurs when the confounding variables correlate with independent variables, including these variables invariably introduces multicolinearity, which causes its own problems including unstable coefficient estimates, lower statistical power and less precise estimates.\n",
    "- Therefore a tradeoff might occur between precision and bias. As you include the formerly omitted variables, you reduce the bias, but the increased multicolinearity can potentially reduce the precision of the estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommendations for addressing Confounding Variables and Omitted Variable Bias\n",
    "- The first thing before you begin your study, arm yourself with all the possible background information you can gather.\n",
    "- Research the study area, review the literature, and consult with experts. This process enables you to identify and measure the crucial variables that you should include in your model and helps avoid the problem in the first place.\n",
    "- After collecting all the data if you realize that a critical variable is missing, it can be very expensive.\n",
    "- After the analysis, this background information can help you identify potential bias, and if necessary track down the solution.\n",
    "- Check the residual plots, sometimes you might not be sure whether bias exists, but the plots can display the hallmarks of confounding variables.\n",
    "- Omitted variable bias might not always be a significant problem as it decreases as the degree of correlations decrease. Understanding the relationships between the variables might help you make this decision.\n",
    "- There is a tradeoff between precision and bias of the estimates that might occur.\n",
    "    - As you add confounding variables to reduce the bias keep an eye on the precision of the estimates.\n",
    "    - To track the precision, check the confidence interval of the estimates.\n",
    "    - If the intervals become wider, the estimates are less precise.\n",
    "    - In the end, you might want to accept a little bias if it significantly improves precision.\n",
    "\n",
    "- If you cannot include an important variable and it causes omitted variable bias, consider using a **proxy variable**.\n",
    "    - **Proxy variables** are easy to measure, and are used instead of variables that are impossible or difficult to measure.\n",
    "    - The proxy variable variable can be a characteristic that is not of any great importance itself, but has a good correlation with the confounding variable.\n",
    "    - These variables allow you to include some of the information in the model that would not have been possible otherwise, and thereby reducing omitted variable bias.\n",
    "- Finally if you can't correct omitted variable bias using any method then you can at least predict the direction of the bias for your estimates.\n",
    "- After identifying confounding variable candidates, you can estimate their theoretical correlations with the relevant variables and predict the direction of the bias.\n",
    "- Always remember that its easy to get stuck on determining which set of candidate variables to include, that you forget which variables you might be excluding without even realizing it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automated Variable Selection Procedures\n",
    "- Automatic variable selection procedures are algorithms that pick the variables to include in your regression model.\n",
    "- Stepwise Regression and best subsets regression are two of the more common variable selection methods.\n",
    "- These automatic procedures can help when you have many independent variables, and you need assistance in the investigative stages of the variable selection process.\n",
    "- These procedures are especially useful when theory and experience provide only a vague sense of which variables you should include in the model.\n",
    "- However if theory and expertise are strong guides, its generally better to follow them than to use an automated procedure.\n",
    "- Additionally, if you use one of these procedures, you should consider it only as the first step of model selection process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stepwise Regression\n",
    "- This procedure begins with a set of candidate independent variables and then adds or removes independent variables one at a time using the variable's statistical significance.\n",
    "- Stepwise either adds the most significant variable or removes the least significant variable.\n",
    "- It does not consider all possible models, and it produces a single regression model when the algorithm ends.\n",
    "- Typically, you can control the specifics of the stepwise procedure, like you can specify whether it can only add variables, only remove variables or both.\n",
    "- You can also set the significance level for including and excluding the independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Subsets Regression\n",
    "- Best subsets regression is also known as \"all possible regressions\" and \"all possible models\".\n",
    "- This method fits all possible models based on the independent variables that you specify.\n",
    "- The number of models that this procedure fits multiplies quickly. It fits $2^P$ models, where P is the number of predictors in the dataset.\n",
    "- So if you have 10 independent variables, it will fit $2^{10} = 1024$ models. If you have 20 variables, it fits $2^{20} = 1,048,576$ models.\n",
    "- After fitting all of the models, best subsets regression then displays the best fitting models with one independent variable, two variables, and so on.\n",
    "- It uses either adjusted R-squared or Mallows' Cp as the criterion for picking the best fitting models for this process.\n",
    "- You need to compare the models to determine which one is the best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stepwise Regression vs Best Subsets Regression\n",
    "- Both the automatic variable selection procedures assess the full set of candidate independent variables that you specify, the end results can be different.\n",
    "- Stepwise regression does not fit all the models but instead assesses the statistical significance of the variables and arrives at a single model.\n",
    "- Best subsets regression fits all possible models and displays some of the best candidates based on adjusted R-squared or Mallows' Cp.\n",
    "- The single model that stepwise regression produces can be simpler to analyze. However, best subsets regression presents more information that is potentially valuable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Stepwise and Best Subsets Regression on the same Dataset\n",
    "- The following example scenario models a manufacturing process. We will determine whether the production conditions are related to the strength of a product.\n",
    "- For both variable selection procedures, we'll use the same independent and dependent variables.\n",
    "- **Dependent Variable** - Strength\n",
    "- **Independent Variable** - Temperature, Pressure, Rate, Concentration, Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Strength</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>Pressure</th>\n",
       "      <th>Rate</th>\n",
       "      <th>Concentration</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>271.8</td>\n",
       "      <td>783.35</td>\n",
       "      <td>33.53</td>\n",
       "      <td>40.55</td>\n",
       "      <td>16.66</td>\n",
       "      <td>13.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>264.0</td>\n",
       "      <td>748.45</td>\n",
       "      <td>36.50</td>\n",
       "      <td>36.19</td>\n",
       "      <td>16.46</td>\n",
       "      <td>14.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>238.8</td>\n",
       "      <td>684.45</td>\n",
       "      <td>34.66</td>\n",
       "      <td>37.31</td>\n",
       "      <td>17.66</td>\n",
       "      <td>15.68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>230.7</td>\n",
       "      <td>827.80</td>\n",
       "      <td>33.13</td>\n",
       "      <td>32.52</td>\n",
       "      <td>17.50</td>\n",
       "      <td>10.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>251.6</td>\n",
       "      <td>860.45</td>\n",
       "      <td>35.75</td>\n",
       "      <td>33.71</td>\n",
       "      <td>16.40</td>\n",
       "      <td>11.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Strength  Temperature  Pressure   Rate  Concentration   Time\n",
       "0     271.8       783.35     33.53  40.55          16.66  13.20\n",
       "1     264.0       748.45     36.50  36.19          16.46  14.11\n",
       "2     238.8       684.45     34.66  37.31          17.66  15.68\n",
       "3     230.7       827.80     33.13  32.52          17.50  10.53\n",
       "4     251.6       860.45     35.75  33.71          16.40  11.00"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the dataset file\n",
    "manufacturing_df = pd.read_csv(\"../../datasets/RegressionAnalysisDatasets/ProductStrength.csv\")\n",
    "manufacturing_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of Stepwise Regression\n",
    "##### Forward Stepwise Regression\n",
    "- Steps for forward stepwise regression\n",
    "    1. Start with an empty list of columns\n",
    "    2. Then add one column and fit the model\n",
    "    3. Check if the p-value of the new column is less than some threshold\n",
    "    4. If it is below the threshold then add the column to the list else continue from step 2 - 4\n",
    "- The final list of columns will all have significant p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Stepwise Regression\n",
    "def forward_stepwise_regression(regression_df, y, x):\n",
    "    pvalue_threshold_in = 0.15\n",
    "    included_columns = []\n",
    "    while True:\n",
    "        feature_found = False\n",
    "        # We remove the column that we have included from the set and repeat the process with remaining columns\n",
    "        excluded_columns = list(set(x) - set(included_columns)) \n",
    "        pvalue_list = pd.Series(index=excluded_columns)\n",
    "        for column_name in excluded_columns:\n",
    "            # add the first column\n",
    "            regression_formula = f\"{y} ~  {' + '.join(included_columns + [column_name])}\"  \n",
    "            model = ols(regression_formula, data=regression_df).fit()\n",
    "            pvalue_list[column_name] = model.pvalues[column_name]\n",
    "        min_pvalue = pvalue_list.min()\n",
    "        if min_pvalue < pvalue_threshold_in:\n",
    "            best_feature = pvalue_list.index[pvalue_list.argmin()]\n",
    "            feature_found = True\n",
    "            included_columns.append(best_feature)\n",
    "            print(f\"Add {best_feature} with p-value {min_pvalue}\")\n",
    "\n",
    "        if not feature_found:\n",
    "            break\n",
    "    return included_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Add Concentration with p-value 5.935009166852511e-09\n",
      "Add Rate with p-value 3.001026934440323e-05\n",
      "Add Pressure with p-value 0.09246550922073958\n",
      "Add Temperature with p-value 0.06676235862897598\n",
      "Final Feature List -\n",
      "['Concentration', 'Rate', 'Pressure', 'Temperature']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\heena\\AppData\\Local\\Temp\\ipykernel_16348\\758623828.py:9: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  pvalue_list = pd.Series(index=excluded_columns)\n",
      "C:\\Users\\heena\\AppData\\Local\\Temp\\ipykernel_16348\\758623828.py:9: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  pvalue_list = pd.Series(index=excluded_columns)\n",
      "C:\\Users\\heena\\AppData\\Local\\Temp\\ipykernel_16348\\758623828.py:9: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  pvalue_list = pd.Series(index=excluded_columns)\n",
      "C:\\Users\\heena\\AppData\\Local\\Temp\\ipykernel_16348\\758623828.py:9: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  pvalue_list = pd.Series(index=excluded_columns)\n",
      "C:\\Users\\heena\\AppData\\Local\\Temp\\ipykernel_16348\\758623828.py:9: FutureWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  pvalue_list = pd.Series(index=excluded_columns)\n"
     ]
    }
   ],
   "source": [
    "y = \"Strength\"\n",
    "x = ['Temperature', 'Pressure', 'Rate', 'Concentration', 'Time']\n",
    "final_feature_list = forward_stepwise_regression(manufacturing_df, y, x)\n",
    "print(f\"Final Feature List -\\n{final_feature_list}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward stepwise regression\n",
    "def backward_stepwise_regression(regression_df, y, x):\n",
    "    pvalue_threshold_out = 0.15\n",
    "    included_columns = x\n",
    "    while True:\n",
    "        feature_eliminated = False\n",
    "        regression_formula = f\"{y} ~  {' + '.join(included_columns)}\"\n",
    "        model = ols(regression_formula, data=regression_df).fit()\n",
    "        # use all p values except intercept\n",
    "        pvalue_list = model.pvalues[1:]\n",
    "        max_pvalue = pvalue_list.max()\n",
    "        if max_pvalue > pvalue_threshold_out:\n",
    "            feature_eliminated = True\n",
    "            worst_feature = pvalue_list.index[pvalue_list.argmax()]\n",
    "            included_columns.remove(worst_feature)\n",
    "            print(f\"Removing feature - {worst_feature} with pvalue {max_pvalue}\")\n",
    "\n",
    "        if not feature_eliminated:\n",
    "            break\n",
    "    return included_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing feature - Time with pvalue 0.1943335490326087\n",
      "Final Feature List -\n",
      "['Temperature', 'Pressure', 'Rate', 'Concentration']\n"
     ]
    }
   ],
   "source": [
    "y = \"Strength\"\n",
    "x = ['Temperature', 'Pressure', 'Rate', 'Concentration', 'Time']\n",
    "final_feature_list = backward_stepwise_regression(manufacturing_df, y, x)\n",
    "print(f\"Final Feature List -\\n{final_feature_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Assess your Candidate Regression Models Thoroughly\n",
    "- If you use stepwise regression or best subsets regression to help pick your model, you need to investigate the candidate models thoroughly.\n",
    "- This entails fitting the candidate models the normal way and checking the residual plots to be sure the fit is unbiased.\n",
    "- You also need to assess the signs and values of the regression coefficients to make sure that they make sense.\n",
    "- These automatic model selection procedures can find chance correlations in the sample data (overfitting) and produce models that don't make sense in the real world.\n",
    "- Automatic model selection procedures can be helpful tools, particularly in the exploratory stage. There can be following problems:\n",
    "    - These procedures can sift through many different models and find correlations that exist by chance in the sample. Assess the results critically and use your expertise to determine whether they make sense.\n",
    "    - These procedures cannot take real-world knowledge into account. The model might not be correct in a practical sense.\n",
    "    - Step wise regression does not always choose the model with the largest R-squared value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of Stepwise Regression\n",
    "- Below are definitions for terms used in the comparison\n",
    "    - **Authentic variables** are the independent variables that truly have a relationship with the dependent variable.\n",
    "    - **Noise variables** are independent variables that do not have an actual relationship with the dependent variable.\n",
    "    - The **correct model** includes all of the authentic variables and excludes all of the noise variables.\n",
    "- The best case scenario for stepwise regression is less number of candidate variables with a large sample size. for ex (4 candidate variables with a sample size of 500 observations).\n",
    "- In this scenario the stepwise regression chooses the correct model 84% of the time, but the scenario is not realistic and accuracy drops from here.\n",
    "- When there are more variables to evaluate, it is harder for stepwise regression to identify the correct model.\n",
    "- Multicolinearity also plays a role in the capability of stepwise regression to choose the correct model. \n",
    "- When independent variables are correlated, it's harder to isolate the individual effect of each variable. This difficulty occurs regardless whether it is a human or computer algorithm trying to identify the correct model.\n",
    "- Stepwise Regression and best subsets regression don't usually select the correct model but they can provide value during the very early, investigative stages of fitting the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
