{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Main Effects and Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Notation (OLS)\n",
    "- The following notation for Ordinary Least Squares Regression (OLS) applies to a regression models for entire populations with k independent variables.\n",
    "- These are ideal models that you will obtain if you could measure the entire population.\n",
    "\n",
    "$Y = \\beta_0 + \\beta_1 X_1 + .... + \\beta_k X_k + \\epsilon$\n",
    "\n",
    "In this notation:\n",
    "- Y represents the dependent variable\n",
    "- The beta ($\\beta$) represents the true population parameters. $\\beta_0$ is the constant while the other betas are for independent variables.\n",
    "- $X$'s are the independent variables.\n",
    "- Epsilon ($\\epsilon$) represents the error, which is the left-over random portion of variability that the model can't explain.\n",
    "\n",
    "- We never work with the entire population. Instead we use samples to estimate the population parameters. The notation for Regression model based on the sample is the following:\n",
    "\n",
    "$\\hat{y} = \\hat{\\beta_0} + \\hat{\\beta_1} X_1 + .... + \\hat{\\beta_k} X_k +\\hat{\\epsilon}$\n",
    "\n",
    "In this notation - The hats represents the sample estimates of the population values.\n",
    "- $\\hat{y}$ - represents the fittted value for the dependent variable. When we enter the values for the independent variable into the regression equation, we obtain the fitted value of the dependent variable.\n",
    "- $\\hat{\\beta}$ - the beta hats represent the estimates of population parameters. These estimates are the `regression coefficients` that appear in your output.\n",
    "- $\\hat{\\epsilon}$ - the epsilon hat represents the estimate of the error or what we call `residuals`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three types of Effects in Regression Models.\n",
    "- **Main Effects** - The relationship between an independent variable and the dependent variable does not depend on the value of other variables in the model.\n",
    "\n",
    "- **Curvilinear Effects** - The relationship between the dependent and independent variable changes based on the value of that independent variable itself. Instead of following a straight line on a graph , these relationship follow curves.\n",
    "\n",
    "- **Interaction Effects** - The relationship between an independent variable and the dependent variable depends on the value of atleast one other independent variable in the model.\n",
    "\n",
    "- For main and interaction effects, the interpretation differs for continuous versus categorical variables. \n",
    "- On the other hand, curved relationships can exist only for continuous data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Effects of Continuous Variables\n",
    "- Main effects for continuous variables are the most common type of relationship in regression models. \n",
    "- For ex we have two independent variables A and B:\n",
    "    - Both the variables are statistically significant, and the model provides a good fit for the data.\n",
    "    - In this scenario it can be concluded that the effect of variable A on the dependent variable(output) doesnot change due to change in values of variable B.\n",
    "    - Also variable A's effect is consistent throughout the range of values for A.\n",
    "    - The same applies to the effect of variable B - It doesnot depend on A, and it remains consistent.\n",
    "    \n",
    "\n",
    "- Coefficients and p values in regression analysis helps to understand which relationships in the model are statistically significant and the nature of those relationships.\n",
    "- The coefficients represent a variable's effect and describe the magnitude and direction of the relationship between each independent variable.\n",
    "- Coefficients are numbers in the regression equation that multiply the values of the variables(slope or m).\n",
    "- The p-values for the coefficients indicate whether these relationships are statistically significant.\n",
    "- The sign of a regression coefficient tells whether the value of dependent variable increases or decreases with change in each independent variable. In other words +ve coefficient means as the value of independent variable increases, the mean for the dependent variables also increases.\n",
    "- A -ve coeffcient means a decrease in mean of independent variable with the increase in value of the dependent variable(inverse relationship).\n",
    "- The coefficent value signifies how much the mean value of dependent variable changes with a unit increase in independet variable, while keeping all the other variables constant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confidence Intervals for Regression Parameters\n",
    "- If we collect a random sample and calculate the mean, the sample mean is the point estimate for population mean.\n",
    "- We will never know the exact value if the population parameter because we will only work with samples.(Not possible to collect population data).\n",
    "- The point estimate doesn't indicate how far from the population parameter it is likely to be.\n",
    "- For this we can calculate the confidence intervals for population parameters.\n",
    "- A `confidence interval` is dervided from a sample and provides a range of values that likely contains the unknown value of population parameter.\n",
    "- Different random samples drawn from the population are likely to produce slightly different intervals.\n",
    "- If we draw many random samples and calculate confidence interval for each sample, a specific proportion of the ranges contains the population parameter. That percentage is the `confidence level`.\n",
    "- For ex - A 95% confidence level suggests that if you draw 20 random samples from the sample population, you'd expect 19 of the confidence intervals to include the population value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The confidence interval provides meaningful estimates because it produces ranges that usually contains the parameter.\n",
    "- We can also see how far our point estimate is likely to be from the parameter value.\n",
    "- In the regression context, we use the sample to calculate the regression coefficients ($\\beta$ hats) which are the point estimates of the population parameters($\\beta$ s).\n",
    "- In the example below the sample estimate of the height coefficient is 106.5. But if we collect multiple samples of the same population, each sample will have its own estimate of the height coefficent. \n",
    "- We won't know the real value or how close to the actual population is our estimate likely to be.\n",
    "- For this we calculate the confidence interval for the regression coefficients as is shown in the cells below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Height M</th>\n",
       "      <th>Weight kg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.60020</td>\n",
       "      <td>49.441572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.65100</td>\n",
       "      <td>62.595751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.65100</td>\n",
       "      <td>75.749931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.53035</td>\n",
       "      <td>48.987979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.45415</td>\n",
       "      <td>43.091278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Height M  Weight kg\n",
       "0   1.60020  49.441572\n",
       "1   1.65100  62.595751\n",
       "2   1.65100  75.749931\n",
       "3   1.53035  48.987979\n",
       "4   1.45415  43.091278"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1. Read the file and create a Data Frame\n",
    "height_weight_df = pd.read_csv(\"../../datasets/RegressionAnalysisDatasets/HeightWeight.csv\")\n",
    "height_weight_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>Weight kg</td>    <th>  R-squared:         </th> <td>   0.497</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.491</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   85.03</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Sun, 03 Sep 2023</td> <th>  Prob (F-statistic):</th> <td>1.74e-14</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>10:47:17</td>     <th>  Log-Likelihood:    </th> <td> -305.70</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    88</td>      <th>  AIC:               </th> <td>   615.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    86</td>      <th>  BIC:               </th> <td>   620.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td> -114.3259</td> <td>   17.443</td> <td>   -6.554</td> <td> 0.000</td> <td> -149.001</td> <td>  -79.651</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Height M</th> <td>  106.5046</td> <td>   11.550</td> <td>    9.221</td> <td> 0.000</td> <td>   83.544</td> <td>  129.465</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 7.927</td> <th>  Durbin-Watson:     </th> <td>   1.176</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.019</td> <th>  Jarque-Bera (JB):  </th> <td>   7.925</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.733</td> <th>  Prob(JB):          </th> <td>  0.0190</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.119</td> <th>  Cond. No.          </th> <td>    45.0</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:              Weight kg   R-squared:                       0.497\n",
       "Model:                            OLS   Adj. R-squared:                  0.491\n",
       "Method:                 Least Squares   F-statistic:                     85.03\n",
       "Date:                Sun, 03 Sep 2023   Prob (F-statistic):           1.74e-14\n",
       "Time:                        10:47:17   Log-Likelihood:                -305.70\n",
       "No. Observations:                  88   AIC:                             615.4\n",
       "Df Residuals:                      86   BIC:                             620.4\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const       -114.3259     17.443     -6.554      0.000    -149.001     -79.651\n",
       "Height M     106.5046     11.550      9.221      0.000      83.544     129.465\n",
       "==============================================================================\n",
       "Omnibus:                        7.927   Durbin-Watson:                   1.176\n",
       "Prob(Omnibus):                  0.019   Jarque-Bera (JB):                7.925\n",
       "Skew:                           0.733   Prob(JB):                       0.0190\n",
       "Kurtosis:                       3.119   Cond. No.                         45.0\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2. Define predictor and response variables\n",
    "\n",
    "x = height_weight_df[\"Height M\"] ## Independent or predictor Variable\n",
    "y = height_weight_df[\"Weight kg\"] ## Dependent or response variable\n",
    "\n",
    "## 3. Add constant to predictor variables\n",
    "ols_model = sm.OLS(y, x).fit()\n",
    "x = sm.add_constant(x)\n",
    "\n",
    "## 4. Fit Linear Regression Model\n",
    "ols_model = sm.OLS(y, x).fit()\n",
    "\n",
    "## 5. View model summary\n",
    "ols_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>const</th>\n",
       "      <td>-149.000505</td>\n",
       "      <td>-79.651360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Height M</th>\n",
       "      <td>83.543896</td>\n",
       "      <td>129.465213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0           1\n",
       "const    -149.000505  -79.651360\n",
       "Height M   83.543896  129.465213"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the confidence interval of the fitted parameters\n",
    "# for reference https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLSResults.conf_int.html\n",
    "ols_model.conf_int(alpha=0.05, cols=None)\n",
    "\n",
    "# NOTE: The confidence interval is based on Student's t distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- From the above confidence interval we can be 95% confident that the actual population value for the height coefficient is between \n",
    "83.5 and 129.5\n",
    "- The width of a confidence interval reveals the precision of the estimate.\n",
    "- Narrower ranges suggest a more precise estimate.\n",
    "- We can evaluate precision by calculating confidence intervals.\n",
    "- If we add more variables to the model and these confidence intervals become wider, then there is a problem because the additional variables reduce the model's precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting p-values for Continuous Independent Variables\n",
    "- Regression Analysis is a form of inferential statistics, where we use a sample to draw conclusions about an entire population.\n",
    "- Sample errors can produce effects in the sample that don't exist in the population.\n",
    "- p-values and significance levels help determine whether the observed relationship in the sample also exist in the larger population.\n",
    "- The p-value for each independent variable tests the null hypothesis that there is no relationship with the dependent variable.\n",
    "- If there is no relationship, then there is no association between the changes in the independent variable and the shifts in the dependent variable.  \n",
    "\n",
    "- The hypothesis for the independent variables are following:  \n",
    "    - **Null Hypothesis** - The coefficient for the independent variable equals zero (there is no relationship) or (p-value > significance level)  \n",
    "\n",
    "    - **Alternative Hypothesis**  - The coefficient for the independent variable doesnot equal zero (there is a relationship) or (p-value < significance level)\n",
    "\n",
    "- If the `p-value` for a variable is less than the significance level, the sample data provides enough evidence to reject the null hypothesis for the entire population.\n",
    "- This means changes in the independent variable are associated with the changes in response at the population level.\n",
    "- The variable is statistically significant and can be included in the regression model. Significance level of 0.05 are the most common value.\n",
    "- If the p-value is greater than significance level, that means there is not enough eveidence in the sample to reject that there is zero correlation and the coeficient doesn't equal to zero.\n",
    "\n",
    "#### Note:\n",
    " - For a refresher on p-values refer to correlation and regression notebook, also go through below videos and exercises.\n",
    "  https://www.khanacademy.org/math/ap-statistics/xfb5d8e68:inference-categorical-proportions/idea-significance-tests/v/p-values-and-significance-tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recoding Continuous Independent Variables\n",
    "- Recoding involves taking original values and mathematically converting them to other values.\n",
    "- While the recoding methods can cause you to interpret some of the results differently, the p-values and goodness-of-fit measures will remmain the same when you fit the same model.\n",
    "- Two common coding methods are 1. Standardization 2. Centering.\n",
    "\n",
    "#### Standardizing the Continuous Variables\n",
    "- To standardize a variable:\n",
    "    - Take each observed value for a variable.\n",
    "    - Subtract the variable's mean.\n",
    "    - Then divide by the variable's standard deviation.\n",
    "- When we standardize a variable, the coded value denotes where the observation falls in the distribution of values.\n",
    "- It indicates the number of standard deviation above or below the variable's mean.\n",
    "- The sign indicates whether the observations are above or below the mean and the number indicates thr number of standard deviations.\n",
    "- Standardized values of 0 indicates that the value is precisely equal to the mean.\n",
    "\n",
    "#### Interpreting Standardized Coefficients\n",
    "- When we fit a model using standardized independent variables, the coefficients are now standardized coefficients.\n",
    "- Standardized coefficients signify the mean change of the dependent variable given a one standard deviation increase in an independent variable.\n",
    "\n",
    "#### Why obtain Standardized Coefficients?\n",
    "- Standardization puts all the variables on the same scale so we can compare the magnitude of the results.\n",
    "- It is used when independent variables have entirely different units like temp(C) and thickness(cm) or it doesn't have any specified units at all like a scale(from 1 to ..).\n",
    "- Standardization puts all the variables on a consistant scale, which allows us to compare the standardized coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Effects of Categorical Variables\n",
    "- Categorical variables, also known as nominal variables, have values that can be put into a countable number of distinct groups based on a characteristic.\n",
    "- For categorical variables we have the variable name and then the level of the variables.\n",
    "- In categorical variables we are dealing with groups of data that we cannot incrementally increase, and therefore it cannot be plotted using a scatterplot.\n",
    "- The levels of variables represent groups in the data and can be plotted using boxplots or barcharts.\n",
    "- Regression analysis estimates mean differences between these groups and determines if they are statistically significant.\n",
    "- Including categorical variables in the regression model allows us to determine, whether the differences in the graph are statistically significant.\n",
    "\n",
    "#### Coding Categorical Variables\n",
    "- To analyze categorical variables, they are converted into indicator variables, also known as dummy variables.\n",
    "- They are columns of 1's and 0's that indicate the presence or absence of a characteristic.\n",
    "- 1 indicates the presence while a 0 represents its absence. \n",
    "- The number of indicator variables depend on the number of categorical levels(or type of categories)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The below table shows the indicator variables created for each level of Gender categorical variable.\n",
    "\n",
    "| Gender | Male | Female |\n",
    "|--------|------|--------| \n",
    "| Male   | 1    | 0      |\n",
    "| Female | 0    | 1      |\n",
    "| Female | 0    | 1      |\n",
    "| Male   | 1    | 0      |\n",
    "\n",
    "- In the table, the Gender column represents the categorical data that we enter into the worksheet.\n",
    "- The value depends on the gender of the subject for which the row corresponds.\n",
    "- The ***Male*** and ***Female*** columns are the ***indicator variables*** based on the gender column.\n",
    "- The Male column contains 1's for observations that correspond to males and 0s for non-males. The same applies to the Female column.\n",
    "- These two columns supply completely redundant information, because one column can predict the other one perfectly.\n",
    "- This is referred to as perfect **multicolinearity**, which will create an error if we enter both the indicator variables in a regression model.\n",
    "- For a categorical variable we must omit one of the underlying indicator variables from the model, which becomes the **reference level**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The below table shows the variable College Major which has 3 levels.\n",
    "\n",
    "| College Major     | Psychology | Political Science | Statistics |\n",
    "|-------------------|------------|-------------------|------------|\n",
    "| Statistics        | 0          | 0                 | 1          |\n",
    "| Psychology        | 1          | 0                 | 0          |\n",
    "| Statistics        | 0          | 0                 | 1          |\n",
    "| Political Science | 0          | 1                 | 0          |\n",
    "| Psychology        | 1          | 0                 | 0          |\n",
    "\n",
    "- In this table, **College Major** is the categorical variable and other columns are indicator variables.\n",
    "- For each row there is a single value of 1, others are 0.\n",
    "- As with the Gender variable if we include all the indicator variables, we are supplying redundant information which will result in error in the model.\n",
    "- If we take any two columns, we can always figure out the value of the third column.\n",
    "- To perform regression analysis we will have to remove one indicator variable, that becomes the reference level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary for Coding Categorical Variables\n",
    "- For all categorical we must remove one level from the analysis and use it as a reference level.\n",
    "- But when we remove one indicator variable, we are altering the data that is being used for fitting the model.\n",
    "- This can change the coefficient and p-values, however using a different reference level does not change the overall story and statistical significance.\n",
    "- If the variable has a natural baseline or a category for comparison, using that level as the reference level will make the interpretation more natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpreting the Results for Categorical Variables\n",
    "- There are several tests that can be performed on categorical variables.\n",
    "\n",
    "**F tests**\n",
    "- Since a categorical variable is represented by multiple indicator variables, we use F tests for that group of indicator variables.\n",
    "- Unlike t-tests, F tests can evaluate multiple model terms simultaneously, which allows them to compare the fits of different linear models.\n",
    "- F-tests calculate the variability within a variable to that of between variables. This way we can check if the changes are significant or due to pure chance.(Watch the video below for more details). \n",
    "- Here the F-test compares the fit of the model with the set of indicator variables that corresponds to a categorical variable with the model without that set of indicator variables.\n",
    "- The **Hypothesis for F-Tests** are as follows:\n",
    "    - ***Null*** : The model with the categorical variable does not improve the fit of the model without the categorical variable.\n",
    "    - ***Alternative*** : The model with the categorical variable fits the data better than the model without categorical variable.\n",
    "\n",
    "- If the p-value is less than the significance level, then we can reject the null hypothesis and conclude that including the categorical variable improves the fit of the model.\n",
    "\n",
    "**t - tests**\n",
    "- While F tests tell us about the categorical variable as a whole, t-tests allows to explore the differences between the group means and the reference level.\n",
    "- The coefficients represent the difference between each level(each category) mean and the reference level mean.\n",
    "- p-value is used to determine whether the difference is statistically significant.\n",
    "- The **Hypothesis for t-Tests** are as follows:\n",
    "    - ***Null*** : The difference between level mean and reference level mean equals zero (not significant)\n",
    "    - ***Alternative*** : The difference between level mean and reference level mean does not equal zero.\n",
    "\n",
    "- If the p-value is less than the significance level than we can reject the null hypothesis and conclude that the level mean is significantly different from reference level mean.\n",
    "- As they are main effects, the sizes of the effects do not change based on the values of other variables in the model. \n",
    "\n",
    "\n",
    "**Further Reading**\n",
    "- Hypothesis Test with F-statistic : https://www.khanacademy.org/math/statistics-probability/analysis-of-variance-anova-library/analysis-of-variance-anova/v/anova-1-calculating-sst-total-sum-of-squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of a model with Categorical Variable\n",
    "- In this part we will analyze the data that includes a categorical variable and a continuous variable.\n",
    "- First we will plot different categories of independent variable and see how they relate to the dependent variable.\n",
    "- Next we will fit the model using OLS regression method, then do ANOVA (Analysis of Variance) on the model.\n",
    "- After the analysis we will interpret the results.\n",
    "\n",
    "**Reference Links for Statsmodel Library**\n",
    "- https://www.statsmodels.org/dev/example_formulas.html\n",
    "- https://www.statsmodels.org/stable/generated/statsmodels.stats.anova.anova_lm.html#statsmodels.stats.anova.anova_lm\n",
    "- https://www.statsmodels.org/stable/anova.html#module-statsmodels.stats.anova\n",
    "- https://www.statsmodels.org/dev/contrasts.html#user-defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Income</th>\n",
       "      <th>Major</th>\n",
       "      <th>Experience</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36669</td>\n",
       "      <td>Political Science</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>24446</td>\n",
       "      <td>Political Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>61115</td>\n",
       "      <td>Political Science</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24446</td>\n",
       "      <td>Political Science</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>24446</td>\n",
       "      <td>Political Science</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>24446</td>\n",
       "      <td>Political Science</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>85561</td>\n",
       "      <td>Political Science</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>24446</td>\n",
       "      <td>Political Science</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>48892</td>\n",
       "      <td>Political Science</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>85561</td>\n",
       "      <td>Psychology</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Income              Major  Experience\n",
       "0   36669  Political Science           4\n",
       "1   24446  Political Science           1\n",
       "2   61115  Political Science           5\n",
       "3   24446  Political Science           1\n",
       "4   24446  Political Science           2\n",
       "5   24446  Political Science           2\n",
       "6   85561  Political Science           7\n",
       "7   24446  Political Science           4\n",
       "8   48892  Political Science           5\n",
       "9   85561         Psychology           5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the data and display it\n",
    "major_income_df = pd.read_csv(\"../../datasets/RegressionAnalysisDatasets/Categorical_Example.csv\", index_col=False)\n",
    "# experience_df = major_income_df[\"Experience\"]\n",
    "# major_income_df.columns\n",
    "major_income_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANOVA (Analysis of Variance) for Major and Experience independent variables.\n",
    "# Step 1 - Group the Data by  Major\n",
    "major_groups = major_income_df.groupby(\"Major\")\n",
    "# Step 2 - Extract Individual Groups\n",
    "major_groups_dict = {}\n",
    "for group_name, group_df in major_groups:\n",
    "    major_groups_dict[group_name] = group_df[[\"Income\", \"Experience\"]]\n",
    "\n",
    "political_science = major_groups_dict[\"Political Science\"]\n",
    "psychology = major_groups_dict[\"Psychology\"]\n",
    "statistics = major_groups_dict[\"Statistics\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "F_onewayResult(statistic=array([2.41589916, 1.97925371]), pvalue=array([0.10833903, 0.15771777]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform the ANOVA using scipy.stats \n",
    "stats.f_oneway(political_science, psychology, statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>Income</td>      <th>  R-squared:         </th> <td>   0.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.205</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3.500</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 14 Sep 2023</td> <th>  Prob (F-statistic):</th>  <td>0.0295</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>07:35:58</td>     <th>  Log-Likelihood:    </th> <td> -339.43</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    30</td>      <th>  AIC:               </th> <td>   686.9</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    26</td>      <th>  BIC:               </th> <td>   692.5</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "                                  <td></td>                                    <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>                                                        <td> 4.906e+04</td> <td> 7469.982</td> <td>    6.568</td> <td> 0.000</td> <td> 3.37e+04</td> <td> 6.44e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Major, Treatment(reference='Statistics'))[T.Political Science]</th> <td>-2.719e+04</td> <td> 9812.758</td> <td>   -2.771</td> <td> 0.010</td> <td>-4.74e+04</td> <td>-7024.312</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>C(Major, Treatment(reference='Statistics'))[T.Psychology]</th>        <td>-5368.3564</td> <td> 9915.559</td> <td>   -0.541</td> <td> 0.593</td> <td>-2.58e+04</td> <td>  1.5e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Experience</th>                                                       <td> 5085.2826</td> <td> 2283.663</td> <td>    2.227</td> <td> 0.035</td> <td>  391.146</td> <td> 9779.419</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.853</td> <th>  Durbin-Watson:     </th> <td>   2.559</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.054</td> <th>  Jarque-Bera (JB):  </th> <td>   4.111</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.743</td> <th>  Prob(JB):          </th> <td>   0.128</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.040</td> <th>  Cond. No.          </th> <td>    10.8</td>\n",
       "</tr>\n",
       "</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                 Income   R-squared:                       0.288\n",
       "Model:                            OLS   Adj. R-squared:                  0.205\n",
       "Method:                 Least Squares   F-statistic:                     3.500\n",
       "Date:                Thu, 14 Sep 2023   Prob (F-statistic):             0.0295\n",
       "Time:                        07:35:58   Log-Likelihood:                -339.43\n",
       "No. Observations:                  30   AIC:                             686.9\n",
       "Df Residuals:                      26   BIC:                             692.5\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "====================================================================================================================================\n",
       "                                                                       coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------------------------------------------------------------\n",
       "Intercept                                                         4.906e+04   7469.982      6.568      0.000    3.37e+04    6.44e+04\n",
       "C(Major, Treatment(reference='Statistics'))[T.Political Science] -2.719e+04   9812.758     -2.771      0.010   -4.74e+04   -7024.312\n",
       "C(Major, Treatment(reference='Statistics'))[T.Psychology]        -5368.3564   9915.559     -0.541      0.593   -2.58e+04     1.5e+04\n",
       "Experience                                                        5085.2826   2283.663      2.227      0.035     391.146    9779.419\n",
       "==============================================================================\n",
       "Omnibus:                        5.853   Durbin-Watson:                   2.559\n",
       "Prob(Omnibus):                  0.054   Jarque-Bera (JB):                4.111\n",
       "Skew:                           0.743   Prob(JB):                        0.128\n",
       "Kurtosis:                       4.040   Cond. No.                         10.8\n",
       "==============================================================================\n",
       "\n",
       "Notes:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Another way to carry out ANOVA test is to first fit the regression model and then calculate the f and p values.\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "model = ols(\"Income ~ Experience + C(Major, Treatment(reference='Statistics'))\", data=major_income_df).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Regression Equation**\n",
    "- To get the regression equation, for this example - The constant is the mean salary for Statistics(49064) which is constant \n",
    "- We can use the coefficients for other categories and find the difference between these groups, which will be unchanged.\n",
    "- The indicator variables will shift the regression line up and down the y-axis for specific groups by the value of the coefficient for the corresponding indicator variable.\n",
    "- We can obtain separate equations for each categorical level with different constants as shown below.\n",
    "\n",
    "| Major             | Equation                                             |\n",
    "|-------------------|------------------------------------------------------|\n",
    "| Political Science | Income = (49064 - 27195 = 21869) + 5085 * Experience |\n",
    "| Psychology        | Income = (49064 - 5368 = 43696) + 5085 * Experience  |\n",
    "| Statistics        | Income = (49064) + 5085 * Experience                 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sum_sq</th>\n",
       "      <th>df</th>\n",
       "      <th>F</th>\n",
       "      <th>PR(&gt;F)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>C(Major, Treatment(reference='Statistics'))</th>\n",
       "      <td>3.762712e+09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.141929</td>\n",
       "      <td>0.027447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Experience</th>\n",
       "      <td>2.252343e+09</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.958681</td>\n",
       "      <td>0.034833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Residual</th>\n",
       "      <td>1.180978e+10</td>\n",
       "      <td>26.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   sum_sq    df         F  \\\n",
       "C(Major, Treatment(reference='Statistics'))  3.762712e+09   2.0  4.141929   \n",
       "Experience                                   2.252343e+09   1.0  4.958681   \n",
       "Residual                                     1.180978e+10  26.0       NaN   \n",
       "\n",
       "                                               PR(>F)  \n",
       "C(Major, Treatment(reference='Statistics'))  0.027447  \n",
       "Experience                                   0.034833  \n",
       "Residual                                          NaN  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ANOVA Result\n",
    "anova_result = sm.stats.anova_lm(model, typ=2)\n",
    "anova_result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation of Example Results\n",
    "- In the above example we want to determine whether college major relates to Income.\n",
    "- We included College Major as a categorical variable with 3 levels - Political Science, Statistics and Psychology.\n",
    "- The analysis will determine whether the mean differences between the groups are statistically significant.\n",
    "- We are also including years of experience as a continuous variable, by adding this variable we can control for differences in the years of experience that might exist between the groups.\n",
    "- If the subjects of one group have more years of experience by chance, the mean (Income) of that group will appear to be higher, but it would be due to experience than due to the major itself.\n",
    "- But by including Experience as a variable, the model controls for that possibility, which means we can learn about income differences by major while keeping experience constant.\n",
    "- Here we assume that we are studying this from the perspective of Statistics department (how other majors compare to statistics), so we use \"Statistics\" as a reference level for the model. This is displayed in the formula where we define it explicitly.\n",
    "\n",
    "#### ANOVA Table\n",
    "- In the ANOVA table in the above cell, we can find the overall significance of the variables.\n",
    "- The p-value for the Experience variable is 0.035 which is less than the significance level of 0.05, which means that the variable is statistically significant.\n",
    "- For the categorical variable Major, it uses 2 degrees of freedom as it has 3 levels and we are using Statistics as reference level, so it is excluded from the model.\n",
    "- The model includes 2 indicator variables to represent the entire categorical variable of Major.\n",
    "- If a categorical variable has many levels, it will use many degrees of freedom.\n",
    "- This is a problem if the sample size is small and can lead to overfitting.\n",
    "- The F-value in the output shows that the variable Major is statistically significant overall. It improves the fit of the model.\n",
    "\n",
    "#### Regression Coefficients\n",
    "- In this we explore the differences in mean income by major by assessing the coefficients.\n",
    "- The model summary table displays the coefficients for Major and Experience.\n",
    "- In the table it displays only two levels for 'Major' variable as we have used Statistics as a reference level, it is excluded from output.\n",
    "- We can choose different reference levels based on the problem statement.\n",
    "- The coefficients of other 2 levels - Political Science and Psychology indicate how mean incomes of these major compare to the mean income of the Statistics major.\n",
    "- As per the result the coefficients are -ve, which means that these Majors have lower mean income than Statistics\n",
    "- We learn the following from the coefficients:\n",
    "    - The mean income for political science majors is $27,195 LESS than the mean income for Statistics majors\n",
    "    - The mean income for psychology majors is $5368 LESS than the mean income for statistics majors.\n",
    "\n",
    "- ***Experience coefficient*** \n",
    "    - For each one year increase in experience, mean income increases by an average of $5085 while holding major constant.\n",
    "    - Conversely the Major holds constant, years of experience. This is useful for isolating the effect of each variable.\n",
    "\n",
    "#### p-values and Significance\n",
    "- The `P>|t|` column in the above table shows the p-values for t-tests.\n",
    "- These p-values determine whether the mean differences are statistically significant.\n",
    "- The Political Science coefficient is statistically significant. So we can reject the null hypothesis that the mean difference is zero.\n",
    "- This means that we are rejecting the notion that the coefficient can plausibly equal 0, even while accounting for random sampling error.\n",
    "- We have sufficient evidence to conclude that these two means are different.\n",
    "- On the other hand the difference between mean incomes for Psychology ans Statistics is not statistically significant(0.593 > 0.05).\n",
    "- This means that we have insufficient evidence to conclude that there is a difference in both means(Stats and Psych).\n",
    "- In other words the value -$5368 might represent random error and if we take another sample and perform analysis the difference might not be there.\n",
    "\n",
    "#### Summary and Overall Significance\n",
    "- If we fit the model using a different reference level, the overall significance of the variable Major in the ANOVA table will remain the same as will the goodness-of-fit measures like R-squared.\n",
    "- The comparisons between specific levels will change as we'd be comparing the majors to a different reference level.\n",
    "- Use the reference level that makes the most sense for the question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Count and Ordinal Variables\n",
    "- There are some scenarios where there is some ambiguity as to whether a variable is continuous or categorical.\n",
    "- In such cases we have the discretion to include a variable as continuous or categorical.\n",
    "- This tends to occur in 2 broad types of scenarios\n",
    "    1. **Count or Ordinal Variable**\n",
    "        - In the first scenario, at least one of your independent variable is a count variable or an ordinal variable.\n",
    "        - These types of variables are discreet, but they contain information about order, scale or magnitude.\n",
    "        - They share properties of both continuous and categorical variables, but aren't quite either one.\n",
    "        - ***Count variables*** are non negative integers. Ex - no of defects, no of days in a hospital and no of treatment sessions.\n",
    "        - ***Ordinal Variables*** have at least 3 categories and the categories have a natural order. The categories are ranked, but the differences between the categories might not be equal.  \n",
    "        For ex - first, second and third in a race are ordinal data. The difference in time between first and second place might not be the same as the difference between second and third place.\n",
    "\n",
    "    2. **Continuous variable with discrete values**\n",
    "        - In the second scenario we have a continuous variable, but it uses only a limited number of discrete chosen values.\n",
    "        - for example, baking cakes at different temperatures of 325, 375 and 425 degrees for a study. or in a longitudinal study, observations occur at specific intervals 1 month, 2 months, 3 months etc.\n",
    "\n",
    "**Note**:  \n",
    "Determining how to include these variables in the model depends on both the nature of your data and the purpose of your study."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Case for including it as a Continuous Variable\n",
    "- When a variable has many levels, it might be best to include it as a continuous variable.\n",
    "- At a bare minimum a variable must have at least 3 values to fit a straight line. However its hard to determine whether there is a linear trend with only 3 values.\n",
    "- Fitting a curved relationship requires more values.\n",
    "- If a study wants to determine how changes in the independent variable relate to changes in the dependent variable, including a variable as a continuous variable allows to determine that type of relationship.\n",
    "\n",
    "#### The Case for including it as a Categorical Variable\n",
    "- If a variable has only a few levels, we can include it as a categorical variable.\n",
    "- In this case, the procedure estimates a fitted mean for each group and does not consider the order of values.\n",
    "- But as the number of values increases, it becomes increasingly unwieldy comparing difference between means.\n",
    "- Additionally coding a categorical variable requires many degrees of freedom when a variable has many levels. This issue is problematic when we have a small sample size.\n",
    "- If a study wants to assess group means and differences between means, including the variable in question as a categorical variable allows to answer these questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constant (Y Intercept)\n",
    "- The constant term in regression analysis is the value at which the regression line crosses the y-axis. The constant is also known as the y-intercept.\n",
    "- The constant is often defined as the mean of the dependent variable when you set all of the independent variables in your model to zero.\n",
    "- In a purely mathematical sense, this definition is correct, but in practice is it impossible to set all the independent variables to zero as this combination can be irrational or doesn't make sense.\n",
    "- The more independent variables you have, the less likely it is that each and every one of them can equal zero simultaneously.\n",
    "- If the independent variables can't all equal zero, we get an impossibly negative value of y-intercept.(see Height-Weight example).\n",
    "- As a general statistical guideline, never make a prediction for a point that is outside the range of observed values that were used to fit the regression model.\n",
    "- A portion of the estimation process for the y-intercept is based on the exclusion of relevant variables from the Regression model.\n",
    "- When we leave relevant variables out, this can produce bias in the model.\n",
    "- ***Bias*** exists if the residuals have an overall positive or negative mean. In other words, then model tends to make predictions that are systematically too high or too low.\n",
    "- The constant term prevents this overall bias by forcing the residual mean to equal zero.\n",
    "- Imagine if we can move the regression line up or down to the point where the residual mean equals zero. This process is how the constant in a regression model ensures, that the residual average equals zero.\n",
    "- However this process does not focus producing a y-intercept that is meaningful for the study area. Thus, the intercept has \"no meaning\" in a regression equation.\n",
    "- The constant ensures that the residuals don't have an overall bias, but that might make it meaningless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Including the constant in a Regression Model\n",
    "- The reason as explained above explains why you should almost always include  the constant in your regression model - it forces the residuals to have zero mean which is important.\n",
    "- If you don't include the constant in your regression model, you are actually setting the constant to zero.\n",
    "- This will force the regression line to go through the origin. In other words, a model that doesn't include the constant requires all of the dependent and independent variables to equal zero simultaneously.\n",
    "- If this isn't correct for the study area, the regression model will exhibit bias without the constant.\n",
    "- When it comes to using and interpreting the constant in a regression model, you should almost always include the constant in the regression model even though it is almost never worth interpreting.\n",
    "\n",
    "**Side Note**  \n",
    "The key benefit of regression analysis is determining how changes in the independent variables are associated with the shifts in the dependent variable. Don't think about the y-intercept too much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
